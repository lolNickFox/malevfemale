619011

research-article2015

JOMXXX10.1177/0149206315619011Journal of ManagementBanks et al. / Questionable Research Practices

Journal of Management Vol. 42 No. 1, January 2016 5-20 DOI: 10.1177/0149206315619011
(c) The Author(s) 2015 Reprints and permissions: sagepub.com/journalsPermissions.nav
Guest Editorial Commentary
Questions About Questionable Research Practices in the Field of Management:
A Guest Commentary
George C. Banks
University of North Carolina at Charlotte
Ernest H. O'Boyle Jr.
University of Iowa
Jeffrey M. Pollack
North Carolina State University
Charles D. White
Longwood University
John H. Batchelor
University of West Florida
Christopher E. Whelpley
Virginia Commonwealth University
Kristie A. Abston
University of West Florida
Andrew A. Bennett
University of Alabama
Cheryl L. Adkins
Longwood University
The discussion regarding questionable research practices (QRPs) in management as well as the broader natural and social sciences has increased substantially in recent years. Despite the attention, questions remain regarding research norms and the implications for both theoretical and practical advancements. The aim of the current article is to address these issues in a question-and-answer format while drawing upon both past research and the results of a series of new studies conducted using a mixed-methods design. Our goal is to encourage a systematic, collegial, and constructive dialogue regarding QRPs in management research.
5

6 Journal of Management / January 2016
Keywords: questionable research practices (QRPs); research methodology; philosophy of science
A recent spate of high profile scandals in the social sciences, some of which occurred within management, has shaken the confidence of those within and outside of the profession, leading to calls for greater transparency and oversight into the research process (Finkel, Eastwick, & Reis, 2015; Matlack, 2013; Schmidt & Hunter, 2015). Adding to concerns about the veracity of management research is evidence that our field and closely related fields may be susceptible to questionable research practices (QRPs; Bosco, Aguinis, Field, Pierce, & Dalton, in press; Francis, Tanzman, & Matthews, 2014; Franco, Malhotra, & Simonovits, 2014; O'Boyle, Banks, & Gonzalez-Mule, in press; Open Science Collaboration, 2015).
QRPs operate in the ambiguous space between what one might consider best practices and academic misconduct. Some examples of QRPs can include presenting post hoc findings as a priori, "cherry picking" fit indices, and selectively deleting outliers for the purpose of achieving statistical significance. The occurrence of these practices is not always questionable; in fact, some of these approaches are beneficial to management research under the right circumstances. For instance, exploratory data analysis has led to numerous discoveries in both the physical and social sciences (for a review, see Locke, 2007). Furthermore, certain fit indices are objectively better than others, and outliers should be examined and, at times, dropped from further analysis (Aguinis, Gottfredson, & Joo, 2013). However, those who see QRPs as a problem point to when the practices are either misreported or not reported rather than the practices themselves (Simmons, Nelson, & Simonsohn, 2011).
The purpose of this commentary is to highlight the wide-ranging perspectives about QRPs in terms of their possible causes, their prevalence, and journal policies that may prevent them.
Editors' Note: This paper was originally submitted as a regular submission. However, the JOM editorial team thought the overall theme and set of issues were better addressed via an editorial commentary aimed at spurring dialogue concerning ethical research practices in our field. We are pleased that the authors have included the full write-up of the five studies that inform this work (downloadable as Supplemental Material; please have a look). As with all journals' editorial policies, JOM's own editorial policy on data transparency, reporting, and other practices discussed in this piece continues to evolve. We have signed on to the Editor's Code of Ethics (https://editorethics.uncc. edu/), are members of the Committee on Publication Ethics (COPE, http://publicationethics.org/), have recently made changes to our review policy in ways that increase reviewer accountability to professional standards, and are considering adopting additional practices and/or partnering with other groups that focus on developing high standards for science and ethics. The current commentary is not an official reflection of JOM's official policies; instead, it is intended to get us all thinking broadly about how we can increase the quality and integrity of our research, together. We will continue to update our readership on ways that JOM is addressing ethical issues in the conduct of research.
--Patrick M. Wright & Deborah E. Rupp
Acknowledgments: Suggestions on earlier versions of this work by Sarai Blincoe, Ken Brown, Sven Kepes, Michael McDaniel, In-Sue Oh, and Steven Rogelberg were valuable in the improvement of this manuscript. We wish to thank Fred Oswald and five anonymous reviewers for their developmental comments as well as the participants in this research for their candor and constructive thoughts.
Supplemental material for this article is available at http://jom.sagepub.com/supplemental
Corresponding author: George C. Banks, Belk College of Business, University of North Carolina at Charlotte, 9201 University City Blvd., Charlotte, NC, 28223, USA.
E-mail: gcbanks@gmail.com

Banks et al. / Questionable Research Practices 7
As we review the literature on QRPs, we provide the results from five new studies. Details on the methods and results of these studies are presented in Supplemental Appendices A through F (see the supplemental material available online). In short, these studies examined engagement in QRPs (Study 1: n = 405 active management researchers), motivations for engaging in and perceptions of the appropriateness of QRPs (Study 2: n = 344 additional active management researchers), current graduate training practices (Study 3: n =126 doctoral students; n = 29 program coordinators), journal guidelines involving QRPs (Study 4: n = 160 management journals), and QRP engagement relative to other fields (Study 5: n = 201 researchers in supply chain management and sociology). In this paper, we integrate the results of these studies and use them to make recommendations. We refer the reader to supplementary readings in several areas of this commentary as space constraints limit the comprehensiveness of discussions.
Background on QRPs
The first mention of QRPs was in the 1958 Code of Professional Ethics and Practices of Public Opinion Researchers. They called upon members to "subscribe to standards imposed everywhere by men of science . . . never to knowingly indulge in questionable research practices in order to arrive at some predetermined result or to `prove' a case" (Riley, 1958: 215). Recent writing characterizes QRPs as design, analytic, or reporting practices that have been questioned because of the potential for the practice to be employed with the purpose of presenting biased evidence in favor of an assertion (Bakker, van Dijk, & Wicherts, 2012; Finkel et al., 2015; John, Loewenstein, & Prelec, 2012; Schmidt & Hunter, 2015; Simmons et al., 2011). Some have also suggested that QRPs be redefined as "questionable reporting practices" (Fanelli, 2013; Sijtsma, Veldkamp, & Wicherts, in press; Wigboldus & Dotsch, in press) to emphasize that the issue is one of transparency. Table 1 presents QRPs that have received the most attention. Despite past efforts to categorize QRPs by level of severity (e.g., Bedeian, Taylor, & Miller, 2010), the rates at which researchers engage in certain QRPs may ultimately make one type of QRP more detrimental than another. For instance, if falsifying data occurs only in very rare circumstances, the problem of the biased reporting of outcomes may be much more detrimental. Ultimately, the extent to which QRPs cause harm is dependent on how the QRPs are used and how the field responds to certain findings.
To examine agreement in management regarding these QRPs, we asked 344 active management researchers about the appropriateness of specific practices (Study 2). For example, when asked about "rounding down" a significance test (e.g., a p value of .054 is reported as p < .05), 13% (95% confidence interval or CI [9%, 17%]) of management researchers disagreed or strongly disagreed that this was inappropriate. Approximately 23% (95% CI [19%, 27%]) of management researchers disagreed or strongly disagreed that selectively reporting hypotheses on the basis of whether they were statistically significant was an inappropriate practice. Additionally, 21% (95% CI [17%, 25%]) of management researchers disagreed or strongly disagreed that excluding data after looking at the impact of doing so on the results was inappropriate. Similar findings were reported for hypothesizing after results are known (HARKing; Kerr, 1998; 25% disagreed or strongly disagreed the practice was inappropriate, 95% CI [20%, 30%]) and selective inclusion or exclusion of control variables to achieve statistical significance (21% disagreed or strongly disagreed, 95% CI [17%, 25%]). In sum, and consistent with past discussions in related literatures on responsible conduct of research, social norms, mentoring, and organizational justice in science (Anderson, Martinson, & De Vries, 2007; Anderson,

8 Journal of Management / January 2016

QRPs (1) Selectively
report hypotheses
(2) Exclude data post hoc
(3) HARKing
(4) Selectively include control variables
(5) Falsify data
(6) "Round off" a p value

Table 1 Definitions of Questionable Research Practices (QRPs)

Description

Sources

Hypotheses with statistically nonsignificant results were less likely to be reported than hypotheses that achieved statistical significance.
A researcher conducts hypothesis testing. Some initial results are not statistically significant. After potential outliers have been removed, some of the initial results become statistically significant.
HARKing or "hypothesizing after results are known" occurs when a researcher analyzes data. After the data analysis, the researcher develops and reports post hoc hypotheses that suggest that findings were defined a priori rather than identified post hoc.
Occurs when a researcher conducts multiple analyses to test the same hypothesis, each time adding or removing different control variables. The researcher reports only the use of control variables that allow for a statistically significant result.
Fabricating a data set rather than engaging in an actual data collection.
Reporting that a p value of .054 is p < .05 rather than p = .05.

Banks, Kepes, & McDaniel (2012); Banks & McDaniel (2011); Bedeian, Taylor, & Miller (2010); Kepes, Banks, McDaniel, & Whetzel (2012); Leung (2011); O'Boyle, Banks, & Gonzalez-Mule (in press); Pigott, Valentine, Polanin, Williams, & Canada (2013); Schmidt & Hunter (2015); Simmons, Nelson, & Simonsohn (2011)
Bedeian et al. (2010); De Vries, Anderson, & Martinson (2006); Kepes & McDaniel (2013); O'Boyle et al. (in press); Schmidt & Hunter (2015); Simmons et al. (2011)
Bedeian et al. (2010); Hitchcock & Sober (2004); John, Loewenstein, & Prelec (2012); Kepes & McDaniel (2013); Kerr (1998); Leung (2011); O'Boyle et al. (in press); Schmidt & Hunter (2015)
John et al. (2012); Kepes & McDaniel (2013); O'Boyle et al. (in press); Simmons et al. (2011)
Bedeian et al. (2010); John et al. (2012); Schmidt & Hunter (2015)
Bakker & Wicherts (2011, 2014); John et al. (2012); Nuijten, Hartgerink, Van Assen, Epskamp, & Wicherts (in press)

Note: The list of references for each QRP is an illustrative list and does not include all authors who have questioned the application of each of these practices.

Ronning, De Vries, & Martinson, 2007; De Vries, Anderson, & Martinson, 2006; Martinson, Anderson, Crain, & De Vries, 2006), our findings showed a lack of consensus among researchers regarding the appropriateness of QRPs. In the following section, we propose a series of questions about QRPs and present evidence from the existing literature as well as the findings from our series of empirical studies on QRPs.

Reviewing the Evidence on QRPs
Question 1: What Causes QRPs?
Researchers have suggested several root causes for QRPs (for reviews, see Bakker et al., 2012; Finkel et al., 2015; John et al., 2012; Simmons et al., 2011). First, it has been suggested that an overemphasis on theory may result in QRPs. For example, Locke writes:

Banks et al. / Questionable Research Practices 9
Everyone who publishes in professional journals in the social sciences knows that you are supposed to start your article with a theory, then make deductions from it, then test it, and then revise the theory. . . . In practice, however, I believe that this policy encourages--in fact demands, premature theorizing and often leads to making up hypotheses after the fact--which is contrary to the intent of hypothetico-deductive method. (2007: 867)
Locke's (2007) assertion is that a rigid adherence to deductive research may lead to engagement in QRPs as it artificially narrows the routes to scientific discovery to not only a priori theory but also a priori theory that is supported. In this way, interesting findings that lack supporting theory (Hambrick, 2007) or are in contrast to "strong theory" may be either suppressed or manipulated to achieve something more consistent with reviewer and reader expectations (Harrison, Banks, Pollack, O'Boyle, & Short, in press).
A second cause for QRPs may be the relative emphasis placed on confirmatory analysis over exploratory analysis (for a more in-depth review, please see Wagenmakers, Wetzels, Borsboom, & Van Der Maas, 2011; Wagenmakers, Wetzels, Borsboom, Van Der Maas, & Kievit, 2012; Wigboldus & Dotsch, in press). For example, confirmatory research often relies on strongly established cutoff points (e.g., p < .05, root-mean-square error of approximation < .08, and comparative fit index > .95) that could encourage some QRPs. Yet exploratory research may also be susceptible to QRPs as researchers may conduct exploratory analyses of their data but report post hoc findings as a priori because of perceptions that confirmatory analyses are superior to exploratory (Kerr, 1998). This sentiment was echoed by an active management researcher we surveyed:
When you consider the style of writing that is expected by JAP [Journal of Applied Psychology], AMJ [Academy of Management Journal], JPSP [Journal of Personality and Social Psychology], and the like then you basically MUST do some of these things. When [have you] last seen a study published that was quantitative and exploratory?
A third and related potential cause of QRPs could be the enormous pressure to publish (Banks & O'Boyle, 2013; Schmidt & Hunter, 2015). With increasing emphasis on journal lists by research-intensive universities, scholars may feel the need to publish to satisfy tenure and accreditation demands as well as to pursue promotion and raises (Bedeian et al., 2010; O'Boyle et al., in press). For instance, one PhD student surveyed wrote,
It's currently impossible to publish anything in a top journal . . . with too many unsupported hypotheses. I don't think that is a good thing, but I do think jobs and tenure are good things, as I currently have neither.
Another participant wrote, "It would be impossible for most researchers to make tenure at any research intensive institution if they were to strictly follow these high standards of research."
If QRPs were simply a pragmatic method of achieving early success in publishing, one would expect that doctoral students and assistant professors are the primary source of QRPs. Similarly, QRPs should primarily be nested in universities with greater research intensity. Yet in the current research, when untenured professors were compared to tenured professors, there were no significant differences in QRP engagement, t(447) = 1.44, p > .05, 95% CI

10 Journal of Management / January 2016
[-0.06, 0.36], Cohen's d = 0.12, nor was there a notable difference in QRP engagement between research-intensive universities and non-research-intensive schools, t(639) = -0.09, p > .05, 95% CI [-0.22, 0.20], Cohen's d = 0.01. If engagement in QRPs is learned during graduate training and pressure to use such practices is encouraged in assistant professors, perhaps the use of such practices could become a habit. Alternatively, as Schmidt and Hunter (2015) suggested, other incentives, such as promotions, raises, and legacy, may motivate QRPs throughout one's career.
Question 2: How Prevalent Are QRPs?
Any attempt to uncover the true base rates of QRPs is obfuscated by the very practices that make them questionable (i.e., lack of reporting or misreporting). However, recent research in management has started to investigate QRP frequency. For instance, O'Boyle et al. (in press) argued that QRPs were the cause for the ratio of supported to unsupported hypotheses more than doubling from defended dissertation to journal publication. Other studies in management have found evidence for QRPs by examining changes from dissertation to published articles (Mazzola & Deuling, 2013), inspecting correlation matrices in published articles (Bosco et al., in press), and probing the accuracy of p-value reporting in multiple moderated regression analyses (O'Boyle, Banks, Carter, Walter, & Yuan, 2015). Using objective measures and observer ratings of engagement in QRPs, these studies provided evidence of potential engagement in practices such as HARKing, suppressing null results, and misreporting p values.
In our surveys of active management researchers (Studies 1 and 2), less than 1% (95% CI [0%, 1%]) reported that they falsified data, and 11% (95% CI [9%, 13%]) said that they had "rounded off" p values. These are relatively low base rates--possibly encouraging to those who believe QRPs cause harm. However, about one third of the researchers stated that for the express purpose of supporting hypotheses with statistical significance, they engaged in post hoc exclusion of data (29%; 95% CI [26%, 32%]) or selectively included control variables (33%; 95% CI [30%, 36%]). Similarly, 50% of management researchers responded that they selectively reported hypotheses on the basis of statistical significance (95% CI [46%, 54%]) and presented a post hoc hypothesis as if it were developed a priori (95% CI [46%, 54%]). Many of the surveyed researchers provided additional insight regarding observations of their own and others' engagement in QRPs. For instance, one wrote:
I've seen top researchers (i.e., SIOP [Society for Industrial and Organizational Psychology] and AOM [Academy of Management] fellows) at top universities (e.g., [redacted]) conduct clear fishing and report only the significant results (about 5% of the runs) at top journals in our field.
A final point related to QRP prevalence is that it has been suggested that those fields with highly developed theoretical paradigms and a lengthy history may have higher rates of QRPs than those fields with a lower level of theory development (Harrison et al., in press). However, we did not observe differences between various subfields, such as OBHR (organizational behavior and human resources) and strategy, t(422) = 1.77, p > .05, 95% CI [-0.04, 0.67], Cohen's d = 0.25. In other words, despite the use of different methods and analytic approaches, we did not see a difference between these management subgroups.

Banks et al. / Questionable Research Practices 11
Question 3: When Are QRPs Learned?
Some researchers (e.g, Bedeian et al., 2010) have suggested that QRPs are learned behaviors that begin in graduate school. Consistent with this sentiment is the following observation by one study participant in the current work:
The moment I started working with PhD students . . . these practices started to surface. They magnified during my training as a PhD, where assistant professors would provide me with a dataset to mine and find "something significant" and, based on it, propose "interesting hypotheses."
If QRPs are simply learned behaviors in doctoral training, then interventions into QRP reduction can target doctoral programs. We surveyed management doctoral students as well as doctoral program coordinators (see Study 3). In the survey, we were interested in both the formal training and the observed behaviors in the program. Overall, 12% (95% CI [6%, 17%]) of PhD students stated that they had observed the practice of "rounding off" p values, and 47% (95% CI [38%, 55%]) said that they were instructed to avoid the practice. Concerning the selective reporting of hypotheses, 55% (95% CI [46%, 63%]) of doctoral students stated they had observed the selective reporting of results, and 61% (95% CI [52%, 69%]) said they had been instructed to avoid the practice. Regarding excluding data post hoc, 33% (95% CI [25%, 41%]) said that they had observed the practice, and 60% (95% CI [52%, 68%]) were instructed to avoid the practice. Furthermore, 58% (95% CI [49%, 67%]) of students indicated that they had observed HARKing, and 71% (95% CI [63%, 79%]) had been instructed to avoid it. For the selective inclusion of control variables to turn a null result into a significant one, 45% (95% CI [36%, 54%]) of PhD students observed the practice, and 52% (95% CI [43%, 61%]) were instructed to avoid it. Finally, 7% (95% CI [3%, 11%]) stated that they had observed data falsification, while 84% (95% CI [78%, 90%]) were instructed to avoid it. The students in this study provided useful insight into their experiences. For instance, one PhD student wrote, "They can teach the ethical practices, but the underlying message is to write something that will get published, as they like to say in my department, it depends how well you `tell the story.'"
In terms of the PhD program coordinators surveyed, roughly half of the responses noted that QRPs represent a "serious" issue, yet some responses claimed that too much is being made of the issue:
As long as there is replication, I don't see this as being an issue.
I think we may have gone too far now in the way we approach these concerns, "witch hunting" is what I see a lot of as an AE [associate editor]. I don't think the concerns are invalid, it's the approach that concerns me.
Thus, there was little consensus regarding the degree to which the field of management should be concerned about QRPs in doctoral training. In terms of preparation, with respect to the design and execution of research, responses illustrated that two mechanisms are currently integrated within the PhD training process. First, the majority of the responses stressed that mentorship (and the advisor-student relationship) is the key mode of training in ethics. In addition, roughly half of the respondents noted that ethics (in research) are covered in classes,

12 Journal of Management / January 2016
brown bags, and special seminars. With regard to how the reporting of results and the publication process are taught, coordinators reported that "mentoring with faculty on research projects" as well as informal "feedback sessions with colleagues and senior staff" were the primary mechanisms for education.
Question 4: Do Researchers Perceive That the Publication Process Encourages QRPs?
Bedeian (2003) surveyed researchers who had published in the Academy of Management Journal and Academy of Management Review, and 25% reported that they were encouraged by editors/reviewers to make changes they believed to be incorrect. Furthermore, experimental research has suggested that reviewers tend to perceive studies with results that achieve statistical significance to have greater relevance and methodological rigor (Emerson, Warme, Wolf, Heckman, Brand, & Leopold, 2010). In the literature on publication bias (for in-depth reviews, see Banks, Kepes, & McDaniel, 2015; Schmidt & Hunter, 2015), there are some data to suggest that authors are the primary cause of this bias, as null results are likely never written up and submitted (see "Myth #2: The Editorial Review Process Is the Primary Cause of Publication Bias" in Banks et al.). Yet there is little evidence that highlights the extent to which editors and reviewers demand the use of QRPs or the biased reporting of outcomes. To assess this, we asked active management researchers whether they had ever been instructed to engage in QRPs by a reviewer or editor. No researchers reported being asked to falsify data, and only 1% (95% CI [0%, 11%]) reported being asked to "round off" a p value. However, 40% (95% CI [32%, 48%]) had been asked to selectively report hypotheses, 10% (95% CI [0%, 20%]) had been asked to exclude data post hoc, 33% (95% CI [25%, 41%]) had been asked to engage in HARKing, and 14% (95% CI [5%, 23%]) had been asked to selectively include control variables. Open-ended responses provided additional clarification. One study participant stated, "In moving through the review process (at a very prestigious journal), the reviewers recommend[ed] that we re-write the hypothesis and include our post hoc analyses as though they were intended as part of our a priori analysis." Another researcher wrote:
If the first story does not appeal, you rewrite it to appease the editors/reviewers. This is a key to getting one's work accepted. Anyone who pretends otherwise is a fool and probably unpublished. Sorry to burst your bubble. This is not science, it is the art of persuasion.
In general, the active management researchers in this study reported that at least some editors and reviewers encourage engagement in QRPs. The notion that reviewers are intolerant of imperfect results (Simmons et al., 2011) and that journals place great emphasis on novel, positive results in pursuit of higher impact factor ratings at the expense of promoting truth is a perspective held by some (Nosek, Spies, & Motyl, 2012). However, this view may create a false dichotomy between those involved in the research process and those involved in the publication process. With rare exception, reviewers and editors are active researchers who are submitting manuscripts to peer-review journals and are likely subjected to the same pressures conducive to QRPs. Nevertheless, the role of gatekeeper may lead reviewers and editors to rationalize QRPs in the hopes of improving the visibility and impact of their journals. For example, an associate editor surveyed wrote:

Banks et al. / Questionable Research Practices 13
Reviewers regularly instruct authors to change post hoc inductive analyses into hypotheses, to avoid unsupported hypotheses, etc. and editors rarely instruct authors against these practices in decision letters. From a pragmatic standpoint, since the pressure to publish is so high, if the keepers of the publishing/research system are asking for science that is not conducted via the scientific method, then I understand why authors would engage in these practices.
While many of the respondents we surveyed reflected this perspective, we offer an optimistic view. Many editors in management have signed the Editor Ethics 2.0 Code (https:// editorethics.uncc.edu/). Efforts such as this are meant to prompt editors to work with authors to reduce the QRPs highlighted above. Furthermore, editors are encouraged to reduce other behaviors, such as promoting self-citations and creating citation cartels with other journals. We see these and other related efforts to be encouraging. For instance, in addition to providing submitting authors with editorial policies, journals may often require authors to affirm that they have acted consistent with certain ethical standards. Here are examples of useful affirmations by some of the leading journals. The Journal of Management provides the following affirmation:
Confirm that the manuscript has been submitted solely to this journal and is not published, in press, or submitted elsewhere.
Confirm that all aspects of this research have been carried out in accordance to the ethical standards of the applicable discipline (e.g., management, psychology), as well as the legal/ ethical standards of the country where both the data were collected and the research carried out.
The Academy of Management Journal provides the following affirmation:
Confirm that all the research meets the ethical guidelines, including adherence to the legal requirements of the study country and obtaining human subjects permission where appropriate. Please see the Academy's Code of Ethics for more information.
And the Journal of Applied Psychology provides the following affirmation:
I agree to comply with APA [American Psychological Association] Ethics Code Standard 8.14a, Sharing Research Data for Verification, allowing other qualified professionals to confirm the analyses and results should my manuscript be accepted for publication. As suggested by APA guidelines, I will retain the raw data for a minimum of 5 years after the publication of this research.
Question 5: What Are Journal Policies Pertaining to QRPs?
The evidence reviewed to this point suggests there are systematic issues leading to engagement in QRPs. Hence, an additional study was designed to explore the extent to which explicitly stated journal policies, in the form of guidance to submitting authors or questions and affirmations during the submission process, addressed the issue of QRPs and encouraged ideal research practices (see Study 4).
Of the 160 management journals we considered, 32% explicitly stated some form of research ethics policy. These policies ranged from the treatment of human subjects

14 Journal of Management / January 2016
to disclosures of potential conflicts of interest. Specifically, 5% discouraged some form of coercive citation practices (e.g., spurious citations meant to improve a journal's impact factor). Furthermore, 17% included policies on data overlap (e.g., the use of the same data in multiple journal submissions), and 23% mentioned plagiarism checks. Roughly 15% of journals included policies about the inclusion of supplemental materials (e.g., additional tables or figures), and 13% mentioned policies about data sharing or data retention (e.g., the APA requirement for sharing data with other professionals for the purpose of confirming analyses). When considering journal policies more specific to engagement in QRPs, journals did not explicitly advise against presenting post hoc hypotheses as a priori. Furthermore, journals did not mention a policy about publishing null results or a policy about reporting all data excluded from an analysis (e.g., outliers). It is important to note that 64% of journals held a Committee on Publication Ethics (COPE) membership and that 27% of management editors signed the Editor Ethics 2.0 Code policies (https://editorethics.uncc.edu/). The Editor Ethics 2.0 Code explicitly states that editors will take action to discourage HARKing and suppressing null results and, in general, will strive to promote transparent reporting. The results illustrate the degree to which management journals have various forms of ethics policies.
Question 6: Does Journal Prestige Influence QRP Engagement?
It has been suggested that the motivation for engaging in QRPs has been to increase one's chance of publishing in prestigious outlets (O'Boyle et al., in press). Past research in a review of major journals has provided evidence that journals with higher impact factors tend to have a higher level of retractions owing in part to QRPs (Fang & Casadevall, 2011). Other investigations into the misreporting of p values and corresponding journal impact factors provide additional inconclusive evidence (Bakker & Wicherts, 2011). In management, empirical evidence has illustrated that engagement in QRPs tends to be correlated with the impact factor of journals in which the research is published (r = .25; O'Boyle et al.). Yet one could argue that QRPs may be used more frequently in lower impact journals where the methodological rigor of studies is potentially weaker.
To further examine this question, we compared authors who had published in higher impact factor journals (see Supplemental Appendix A, Study 1) with those that had published in lower impact factor journals (see Supplemental Appendix B, Study 2). For Study 1, anonymous surveys were sent to authors of articles in 10 journals. For Study 2, similar surveys were sent to authors of articles in another 10 journals with lower impact factors. The findings showed that total engagement in QRPs was likely to be higher for those researchers who published in Study 1 compared to those in Study 2, t(739) = 2.10, p = .04, 95% CI [0.13, 0.40], Cohen's d = 0.15. More specifically, the findings showed that participants in Study 1 were more likely to selectively report findings. However, there was no difference for the other specific QRPs. Anecdotal evidence provided by participants in the open-ended response section provided some support. One researcher wrote:
HARKING is a requirement of our journals, and the more prestigious the more it is emphasized. You MUST MUST MUST MUST invent some theory to explain your findings, and inductive research won't be published.
In sum, there does seem to be evidence that QRPs are used more frequently in articles published in more prestigious journals. While this finding is consistent with past research

Banks et al. / Questionable Research Practices 15
(e.g., Fang & Casadevall, 2011; O'Boyle et al., in press), we do acknowledge the limitation that, in Study 2, participants were also asked whether they believed their actions to be appropriate. It is not clear whether the introduction of this additional question may have influenced participant responses.
Question 7: How Does Engagement in QRPs in the Field of Management Compare to Other Academic Areas in Business and the Broader Social Sciences?
Past reviews have suggested that there may be a greater potential for engagement in QRPs in behavioral science where small sample laboratory experiments are used more frequently (John et al., 2012; Schmidt & Hunter, 2015). A meta-analytic study by Fanelli (2009) suggested that approximately 2% of scientists across fields have either falsified or modified data and that 34% have employed other QRPs. Comparably, less than 2% of psychologists stated that they had falsified data, and approximately one third specified that they had presented post hoc hypotheses as if they had been developed a priori (John et al.). John et al. also estimated that approximately 22% of psychologists reported that they had "rounded off" p values, and 40% excluded data post hoc. When comparing these results to the findings from management in the current study, in general, there seems to be a reasonable degree of overlap. We also compared the field of management to two related social science fields: supply chain and sociology (see Study 5). In general, the evidence suggests that supply chain and sociology are similar to management research in some ways. However, management researchers did report engaging in QRPs to a greater extent. Furthermore, more management researchers, than supply chain and sociology researchers, reported that they were encouraged by reviewers to inappropriately round p values, engage in HARKing, and selectively report hypotheses.
Recommendations for Promoting Transparency, Openness, and Reproducibility
QRPs are suboptimal practices that appear to occur at a nonideal frequency in management research. Moving from a descriptive to prescriptive approach, we close with a discussion of recommendations meant to promote transparency, openness, and reproducibility. These recommendations are focused not only on discouraging QRPs but also on promoting greater sharing and collaboration within our field (for review papers on these issues, see Fang & Casadevall, 2015; Nosek & Bar-Anan, 2012; Nosek et al., 2012).
Recommendations for Management Journals
As one study participant wrote, "Authors should be encourage[d] to be honest and open about all the natural quirks and iterative steps taken as part of the complex endeavor of research." We agree that much of what makes QRPs questionable is the misreporting or lack of reporting of certain behaviors and that journals can play a key role in any reform efforts. A PhD student suggested that "change must occur top-down. Get journals to publish articles containing both significant and insignificant findings, get tenure to be less about `5 articles

16 Journal of Management / January 2016
in A journals,' and the appropriate behaviors will follow." As such, many past recommendations directly deal with standards of reporting in journals that promote transparency, openness, and reproducibility (Finkel et al., 2015; Kepes & McDaniel, 2013; Nosek & Bar-Anan, 2012; Nosek et al., 2012).
This topic was the point of emphasis in a recent article published in Science (Nosek et al., 2015) that presents a series of recommendations for the policies and practices of journals. The eight standards highlighted pertained to (1) citation standards, (2) data transparency, (3) analytic methods (code) transparency, (4) research materials transparency, (5) design and analysis transparency, (6) preregistration of studies, (7) preregistration of analysis plans, and (8) replication. It is important to note that the guidelines have the advantage that there are four levels for each standard to facilitate flexibility and customization by journals. Level 1 is intended to require little to no effort for adoption. Consider data transparency as an example. To date, many efforts by journals to encourage data sharing have been ineffective (Alsheikh-Ali, Qureshi, Al-Mallah, & Ioannidis, 2011; Eich, 2014; Wicherts, Borsboom, Kats, & Molenaar, 2006). At a basic level, journals might simply request that all authors state in their manuscripts whether the data from the study are available for dissemination. If the data cannot be shared, authors could provide a reason (e.g., proprietary rights; confidentiality). Approximately 13% of management journals have an explicitly stated policy about data sharing or retention (see Study 4). Adoption of Level 1 would mean that 100% of journals would have a stated policy while not dictating the terms of that policy.
Level 2 is associated with higher standards. In the context of data sharing, a journal might switch to a default standard that requires data sharing where data are placed in an online repository. An embargo of 1 to 3 years may be included as a temporary restriction that would allow the author(s) an additional opportunity to use the data before the data became publicly available. Exceptions would still be allowed through a formal appeal to the editor at the point of initial submission. Level 3 is the strongest standard but is also the most difficult to implement and would not always be applicable. In the context of data sharing, a journal might require that data be deposited in a trusted repository, and an independent party might reproduce the analyses with the data prior to publication. A fourth level, Level 0, was provided as a point of comparison for journal policies that do not address the transparency standards advocated in the guidelines. The link to the full standards and list of journal signatories committed to considering the guidelines, including many signatories from the field of management, is provided for interested readers (http://centerforopenscience.org/top/).
Recommendations for Management Graduate Training
One participant in our studies wrote,
I generally avoid using significance testing if I can at all help it (the associated perils of NHST [null hypothesis significance testing] were stressed throughout my graduate training). It was interesting how easy it made it to check "never" to many of the questions you posed.
This comment illustrates the importance of graduate training. Scholars have consistently called for an increase in teaching professional ethics in management, often focused at the graduate level (e.g., Wiese & Fullick, 2014). At this level, we contend that professional ethics should extend beyond legal ethics and protection of human subjects to include discussions

Banks et al. / Questionable Research Practices 17
about research design, analysis, and reporting ethics. Explicitly incorporating a discussion of QRPs into a doctoral course could be accomplished when teaching topics like research design, data analysis, and institutional review board (IRB) training. We know that training individuals in QRP awareness leads to having improved ethical reasoning skills and enhanced awareness of ethical issues after formal training; however, this improvement may not last over time. Consequently, QRPs should also be an explicit part of the conversation among experienced scholars. At the university level, colleagues can informally engage others, or departments can formally discuss this at seminars or meetings. At the professional level, seminars, themes at conferences or meetings, and presidential addresses (e.g., Cortina, 2015) can help enhance this topic's visibility.
Recommendations for the Role of Individual Management Researchers
It is important to note that individual researchers do not have to wait for journal policies to change to start promoting open scientific practices. For example, when conducting the latter series of studies for the current work, the studies, materials, and hypotheses were preregistered. The following steps were taken: (1) After receiving approval from the IRB, but prior to data collection, we preregistered the information for the studies via the Open Science Framework operated by the Center for Open Science (https://cos.io/); (2) an anonymized link was created that provides a time stamped "frozen picture" of the studies' information (https:// osf.io/5thpc/?view_only=41843b91eea145ab939097c0f5ae1409); and (3) the preregistration included a priori hypotheses, supplemental research questions, the exact surveys to be used, as well as the design of the study. Preregistration by no means limits the exploration of one's data, nor does preregistration prevent researchers from deviating from the original plan. If changes are made, these changes are documented and can simply be explained.
Individual researchers should also consider the approaches they take to collaboration. There are growing concerns about the issue of "gift authorship" or "hyperauthorship" in the natural and social sciences in which one might wonder whether all authors really provided an intellectual contribution. For instance, there was a recent paper published in physics with 5,154 authors (Hotz, 2015). Whether "gift authorship" is an issue in management is unclear. There are other potential QRPs pertaining to author collaboration or lack thereof. For instance, some have argued that researchers work too much in isolation, leading to a disconnected literature. Others have also argued that dividing research efforts into multiple publications may also present an ethical problem (Elsevier, 2015). In this paper, we believe that the important contribution of this research would not have been possible without a large-scale collaboration. We encourage future research to continue to explore the potential of largescale collaboration while simultaneously respecting APA guidelines regarding intellectual contributions.
Finally, an anonymous reviewer asked whether QRPs would be less of a concern if researchers abandoned NHST for nonfrequentist approaches, such as Bayesian analysis (Zyphur & Oswald, 2015). Our answer is "it depends." QRPs can occur regardless of the analytic framework used. For example, a priori specifications must be made in hypothesis testing whether one uses frequentist or Bayesian approaches (Andraszewicz, Scheibehenne, Rieskamp, Grasman, Verhagen, & Wagenmakers, 2014). If researchers explore their data and then test hypotheses using the same data, they deviate from both the frequentist and the

18 Journal of Management / January 2016
nonfrequentist perspective. Currently, the heavy focus on p values when considering QRPs is "just the tip of the iceberg" (Leek & Peng, 2015: 612), and more work needs to be completed concerning how different analytical approaches may influence QRPs.
Conclusion
We reviewed common questions regarding QRPs in the field of management to help researchers better understand the means for improving general research practices. The collected evidence illustrates engagement in QRPs. The results also shed light on the role of editors, reviewers, graduate training, and journal policies in addressing QRPs. Furthermore, we made recommendations for management journals, graduate training, and individual management researchers to reduce QRPs. Throughout the manuscript, and indirectly through our bibliography, we directed readers to additional resources on the topic. We wish to end with the following quote from one study participant:
We could make the practice of research better if we aimed to define and educate our community about when each practice is or is not reasonable to use. In some cases, it is not questionable but clearly unacceptable--whereas in other cases, it may be a reasonable interpretation because of the separation between the data generation process and the analysis. That is a valuable conversation to have.
We, too, believe that a systematic, collegial, and constructive dialogue around the discussion of QRPs in the management literature is valuable. As this conversation evolves, multiple directions for future work will emerge. We provide thoughts on the limitations of the present work and directions for future research in Supplemental Appendix F.
References
Aguinis, H., Gottfredson, R. K., & Joo, H. 2013. Best-practice recommendations for defining, identifying, and handling outliers. Organizational Research Methods, 16: 270-301.
Alsheikh-Ali, A. A., Qureshi, W., Al-Mallah, M. H., & Ioannidis, J. P. 2011. Public availability of published research data in high-impact journals. PLoS ONE, 6(9): e24357. http://journals.plos.org/plosone/article?id=10.1371/ journal.pone.0024357
Anderson, M. S., Martinson, B. C., & De Vries, R. 2007. Normative dissonance in science: Results from a national survey of US scientists. Journal of Empirical Research on Human Research Ethics, 2: 3-14.
Anderson, M. S., Ronning, E. A., De Vries, R., & Martinson, B. C. 2007. The perverse effects of competition on scientists' work and relationships. Science and Engineering Ethics, 13: 437-461.
Andraszewicz, S., Scheibehenne, B., Rieskamp, J., Grasman, R., Verhagen, J., & Wagenmakers, E.-J. 2014. An introduction to Bayesian hypothesis testing for management research. Journal of Management, 41: 521-543.
Bakker, M., van Dijk, A., & Wicherts, J. M. 2012. The rules of the game called psychological science. Perspectives on Psychological Science, 7: 543-554.
Bakker, M., & Wicherts, J. M. 2011. The (mis)reporting of statistical results in psychology journals. Behavior Research, 43: 666-678. doi:10.3758/s13428-011-0089-5
Bakker, M., & Wicherts, J. M. 2014. Outlier removal and the relation with reporting errors and quality of psychological research. PLoS ONE, 9(7): e103360. http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103360
Banks, G. C., Kepes, S., & McDaniel, M. A. 2012. Publication bias: A call for improved meta-analytic practice in the organizational sciences. International Journal of Selection and Assessment, 20: 182-196. doi:10.1111/ j.1468-2389.2012.00591.x
Banks, G. C., Kepes, S., & McDaniel, M. A. 2015. Publication bias: Understand the myths concerning threats to the advancement of science. In C. E. Lance & R. J. Vandenberg (Eds.), More statistical and methodological myths and urban legends: 36-64. New York: Routledge.

Banks et al. / Questionable Research Practices 19
Banks, G. C., & McDaniel, M. A. 2011. The kryptonite of evidence-based I-O psychology. Industrial and Organizational Psychology: Perspectives on Science and Practice, 4: 40-44. doi:10.1111/j.17549434.2010.01292.x
Banks, G. C., & O'Boyle, E. H., Jr. 2013. Why we need industrial-organizational psychology to fix industrialorganizational psychology. Industrial and Organizational Psychology: Perspectives on Science and Practice, 6: 291-294.
Bedeian, A. G. 2003. The manuscript review process: The proper roles of authors, referees, and editors. Journal of Management Inquiry, 12: 331-338. doi:10.1177/1056492603258974
Bedeian, A. G., Taylor, S. G., & Miller, A. N. 2010. Management science on the credibility bubble: Cardinal sins and various misdemeanors. Academy of Management Learning & Education, 9: 715-725. doi:10.5465/ amle.2010.56659889
Bosco, F. A., Aguinis, H., Field, J. G., Pierce, C. A., & Dalton, D. R. in press. HARKing's threat to organizational research: Evidence from primary and meta-analytic sources. Personnel Psychology. doi:10.1111/peps.12111
Cortina, J. M. 2015. A revolution with a solution. Opening plenary presented at the meeting of the Society for Industrial/Organizational Psychology, Philadelphia.
De Vries, R., Anderson, M. S., & Martinson, B. C. 2006. Normal misbehavior: Scientists talk about the ethics of research. Journal of Empirical Research on Human Research Ethics, 1: 43-50.
Eich, E. 2014. Business not as usual. Psychological Science, 25: 3-6. Elsevier. 2015. Publishing and research ethics: Discussing the major types of scientific misconduct and how to
avoid them. https://www.publishingcampus.elsevier.com/pages/63//ethics/Publishing-ethics.html. Accessed August 20, 2015. Emerson, G. B., Warme, W. J., Wolf, F. M., Heckman, J. D., Brand, R. A., & Leopold, S. S. 2010. Testing for the presence of positive-outcome bias in peer review: A randomized controlled trial. Archives of Internal Medicine, 170: 1934-1939. doi:10.1001/archinternmed.2010.406 Fanelli, D. 2009. How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data. PLoS ONE, 4(5): e5738. http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738 Fanelli, D. 2013. Redefine misconduct as distorted reporting. Nature, 494: 149. Fang, F. C., & Casadevall, A. 2011. Retracted science and the retraction index. Infection and Immunity, 79: 3855-3859. Fang, F. C., & Casadevall, A. 2015. Competitive science: Is competition ruining science? Infection and Immunity, 83: 1229-1233. Finkel, E. J., Eastwick, P. W., & Reis, H. T. 2015. Best research practices in psychology: Illustrating epistemological and pragmatic considerations with the case of relationship science. Journal of Personality and Social Psychology, 108: 275-297. Francis, G., Tanzman, J., & Matthews, W. J. 2014. Excess success for psychology articles in the journal Science. PLoS ONE, 9(12): e114255. http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0114255 Franco, A., Malhotra, N., & Simonovits, G. 2014. Publication bias in the social sciences: Unlocking the file drawer. Science, 345: 1502-1505. Hambrick, D. C. 2007. The field of management's devotion to theory: Too much of a good thing? Academy of Management Journal, 50: 1346-1352. Harrison, J. S., Banks, G. C., Pollack, J. M., O'Boyle, E. H., Jr., & Short, J. C. in press. Publication bias in strategic management research. Journal of Management. doi:10.1177/0149206314535438 Hitchcock, C., & Sober, C. 2004. Prediction versus accommodation and the risk of overfitting. The British Journal for the Philosophy of Science, 55: 1-34. Hotz, R. L. 2015. How many scientists does it take to write a paper? Apparently, thousands. Wall Street Journal. http:// www.wsj.com/articles/how-many-scientists-does-it-take-to-write-a-paper-apparently-thousands-1439169200. Accessed November 11, 2015. John, L. K., Loewenstein, G., & Prelec, D. 2012. Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23: 524-532. doi:10.1177/0956797611430953 Kepes, S., Banks, G. C., McDaniel, M. A., & Whetzel, D. L. 2012. Publication bias in the organizational sciences. Organizational Research Methods, 15: 624-662. doi:10.1177/1094428112452760 Kepes, S., & McDaniel, M. A. 2013. How trustworthy is the scientific literature in I-O psychology? Industrial and Organizational Psychology: Perspectives on Science and Practice, 6: 252-268. Kerr, N. L. 1998. HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2: 196-217. doi:10.1207/s15327957pspr0203_4 Leek, J. T., & Peng, R. D. 2015. P values are just the tip of the iceberg. Nature, 520: 612.

20 Journal of Management / January 2016
Leung, K. 2011. Presenting post hoc hypotheses as a priori: Ethical and theoretical issues. Management and Organizational Review, 7: 471-479.
Locke, E. A. 2007. The case for inductive theory building. Journal of Management, 33: 867-890. Martinson, B. C., Anderson, M. S., Crain, A. L., & De Vries, R. 2006. Scientists' perceptions of organizational
justice and self-reported misbehaviors. Journal of Empirical Research on Human Research Ethics, 1: 51-66. Matlack, C. 2013. Research fraud allegations trail a German B-school wunderkind. Bloomberg Businessweek. http://
www.businessweek.com/articles/2013-06-24/research-fraud-allegations-trail-a-german-b-school-wunderkind. Accessed November 11, 2015. Mazzola, J. J., & Deuling, J. K. 2013. Forgetting what we learned as graduate students: HARKing and selective outcome reporting in I-O journal articles. Industrial and Organizational Psychology: Perspectives on Science and Practice, 6: 279-284. Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S., Buck, S., Chambers, C., Chin, G., Christensen, G., Contestabile, M., Dafoe, A., Eich, E., Freese, J., Glennerster, R., Goroff, D., Green, D. P., Heese, B., Humphreys, M., Ishiyama, J., Karlan, D., Kraut, A., Lupia, A., Marbry, P., Madon, T., Malhotra, N., Wilson, E. M., McNutt, M., Miguel, E., Paluck, E. L., Simonsohn, U., Soderberg, C., Spellman, B. A., Tornow, J., Turitto, J., VandenBos, G. R., Vazire, S., Wagenmakers, E. J., Wilson, R., & Yarkoni, T. 2015. Promoting an open research culture: Author guidelines for journals to promote transparency, openness, and reproducibility. Science, 348: 1422-1425. Nosek, B. A., & Bar-Anan, Y. 2012. Scientific utopia: I. Opening scientific communication. Psychological Inquiry, 23: 217-243. Nosek, B. A., Spies, J. R., & Motyl, M. 2012. Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science, 7: 615-631. doi:10.1177/1745691612459058 Nuijten, M. B., Hartgerink, C. H. J., Van Assen, M. A. L. M., Epskamp, S., & Wicherts, J. M. in press. The prevalence of statistical reporting errors in psychology (1985-2013). Behavior Research Methods. doi:10.3758/ s13428-015-0664-2 O'Boyle, E. H., Banks, G. C., Carter, K., Walter, S., & Yuan, Z. 2015. A 20-year review of outcome reporting bias in moderated multiple regression. Paper presented at the annual meeting of the Academy of Management, Vancouver. O'Boyle, E. H., Jr., Banks, G. C., & Gonzalez-Mule, E. in press. The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. Journal of Management. doi:10.1177/0149206314527133 Open Science Collaboration. 2015. Estimating the reproducibility of psychological science. Science, 349: 6251. doi:10.1126/science.aac4716 Pigott, T. D., Valentine, J. C., Polanin, J. R., Williams, R. T., & Canada, D. D. 2013. Outcome-reporting bias in education research. Educational Researcher, 42: 424-432. doi:10.3102/0013189X13507104 Riley, J. W. 1958. Proceedings of the thirteenth conference on public opinion research. Public Opinion Quarterly, 22: 169-216. Schmidt, F. L., & Hunter, J. E. 2015. Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.). Thousand Oaks, CA: Sage. Sijtsma, K., Veldkamp, C. L., & Wicherts, J. M. in press. Improving the conduct and reporting of statistical analysis in psychology. Psychometrika. doi:10.1007/s11336-015-9444-2 Simmons, J. P., Nelson, L. D., & Simonsohn, U. 2011. False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22: 1359-1366. doi:10.1177/0956797611417632 Wagenmakers, E.-J., Wetzels, R., Borsboom, D., & Van Der Maas, H. L. 2011. Why psychologists must change the way they analyze their data: The case of psi: Comment on Bem (2011). Journal of Personality and Social Psychology, 100: 426-432. Wagenmakers, E.-J., Wetzels, R., Borsboom, D., Van Der Maas, H. L., & Kievit, R. A. 2012. An agenda for purely confirmatory research. Perspectives on Psychological Science, 7: 632-638. Wicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. 2006. The poor availability of psychological research data for reanalysis. American Psychologist, 61: 726-728. Wiese, C. W., & Fullick, J. M. 2014. The fantastic four years: Recommendations for industrial-organizational programs. Industrial and Organizational Psychology: Perspectives on Science and Practice, 7: 21-26. Wigboldus, D. H., & Dotsch, R. in press. Encourage playing with data and discourage questionable reporting practices. Psychometrika. doi:10.1007/s11336-015-9445-1 Zyphur, M. J., & Oswald, F. L. 2015. Bayesian estimation and inference: A user's guide. Journal of Management, 41: 390-420.

